from typing import Optional, Callable, Any
import math
import torch
import torch.optim as optim

__all__ = ['Ranger']


def centralized_gradient(x: torch.Tensor, use_gc: bool = True, gc_conv_only: bool = False) -> torch.Tensor:
    """
    Reference:
    - https://github.com/Yonghongwei/Gradient-Centralization

    :param x: the input gradient.
    :param use_gc: whether to use Gradient Centralization (GC) or not. Default: True.
    :param gc_conv_only: whether to apply GC to conv1d layers only or conv1d + fc layers. Default: False.
    :return: the normalized gradient.
    """
    if use_gc:
        if gc_conv_only:
            if len(list(x.size())) > 3:
                x.add_(-x.mean(dim=tuple(range(1, len(list(x.size())))), keepdim=True))
        else:
            if len(list(x.size())) > 1:
                x.add_(-x.mean(dim=tuple(range(1, len(list(x.size())))), keepdim=True))
    return x


class Ranger(optim.Optimizer):
    """
    The Ranger optimizer combines two new Optimizer into a single optimizer:
    1. RAdam which is a variant of the Adam stochastic optimizer that introduces a term to rectify the variance of the adaptive learning rate.
       RAdam provides the best base for an optimizer to build on as it leverages a dynamic rectifier to adjust the adaptive momentum of Adam
       based on the variance.
    2. Lookahead which iteratively updates two sets of weights. Intuitively, the algorithm chooses a search directly by looking ahead at the
       sequence of fast weight generated by another optimizer. Lookahead provides a breakthrough in robust and stable exploration during the
       entirety of training.
    Gradient centralization (GC) is a new optimization technique which operates directly on gradients by centralizing the gradient vector to have zero-mean.
    GC can be viewed as a projected gradient descent method with a constrained loss function. GC can regularize both the weight space and output feature space
    so that it can boost the generalization performance of DNNs.
    GC improve the Lipschitzness of the loss function and its gradient so that the training process become more efficient and stable. GC is simple to implement
    and can be easily embedded into existing gradient-based DNN optimizers with only one line of code. It can be directly used to fine-time the pre-trained DNNs.

    Reference:
    - https://github.com/lessw2020/Ranger-Deep-Learning-Optimizer
    """

    def __init__(self,
                 params,
                 lr: float = 1e-3,
                 alpha: float = .5,
                 k: int = 6,
                 n_sma_threshold: int = 5,
                 betas: tuple = (.95, .999),
                 eps: float = 1e-5,
                 weight_decay: float = 0,
                 use_gc: bool = True,
                 gc_conv_only: bool = False,
                 gc_loc: bool = True):
        """

        :param params: network parameters.
        :param lr: the learning rate. Default: 1e-3.
        :param alpha: the Ranger option - slow update rate. Default: .5.
        :param k: the Ranger option - lookahead steps. Default: 6.
        :param n_sma_threshold: the Ranger option - adjustable threshold. Default: 5.
        :param betas: the Adam option -beta1 & beta2. Default: (0.95, 0.999).
        :param eps: the Adam option - epsilon value. Default: 1e-5.
        :param weight_decay: the Adam option - weight decay. Default: 0.
        :param use_gc: whether to use Gradient Centralization (GC) or not. Default: True.
        :param gc_conv_only: whether to apply GC to conv1d layers only or conv1d + fc layers. Default: False.
        :param gc_loc: Default: True.
        """

        # parameter checks
        if not 0. <= alpha <= 1.:
            raise ValueError(f"Expected slow update rate 0. <= 'alpha' <= 1., but got 'alpha' = {alpha}.")
        if k < 1:
            raise ValueError(f"Expected lookahead steps 'k' <= 1, but got k = {k}.")
        if lr <= 0:
            raise ValueError(f"Expected learning rate lr > 0, but got 'lr' = {lr}.")
        if eps <= 0:
            raise ValueError(f"Expected epsilon eps <= 0, but got 'eps' = {eps}.")

        # prepare defaults and init torch.optim base
        defaults = dict(lr=lr,
                        alpha=alpha, k=k, n_sma_threshold=n_sma_threshold, step_counter=0,
                        betas=betas, eps=eps, weight_decay=weight_decay)
        super(Ranger, self).__init__(params, defaults)

        # adjustable threshold
        self.n_sma_threshold = n_sma_threshold

        # lookahead params
        self.alpha = alpha
        self.k = k

        # RAdam buffer for state
        self.radam_buffer = [[None, None, None] for _ in range(10)]

        # to use GC or not
        self.use_gc = use_gc
        self.gc_loc = gc_loc
        self.gc_conv_only = gc_conv_only

        # level of gradient centralization
        # self.gc_gradient_threshold = 3 if gc_conv_only else 1

        # print options
        print(f"Ranger optimizer loaded.\nGradient Centralization usage = {self.use_gc}")
        if self.use_gc and (not self.gc_conv_only):
            print(f"GC applied to both conv1d and fc layers.")
        elif self.use_gc and self.gc_conv_only:
            print(f"GC applied to conv1d layers only.")

    def __setstate__(self, state):
        print('Set state called...')
        super(Ranger, self).__setstate__(state)

    def step(self, closure: Optional[Callable[..., Any]] = None) -> float:
        loss = None

        # note - below is commented out b/c I have other work that passes back the loss as a float, and thus not a callable closure.
        # Uncomment if you need to use the actual closure...
        if closure is not None:
            loss = closure()

        # evaluate averages and gradient, update param tensors
        for group in self.param_groups:
            for p in group['params']:
                if p.grad is None:
                    continue
                grad = p.grad.data.float()
                if grad.is_sparse:
                    raise RuntimeError('Ranger optimizer does not support gradients.')
                p_data_fp32 = p.data.float()

                # get state dict from this param
                state = self.state[p]

                if len(state) == 0:
                    # if first time to run...init dictionary with our desired entries
                    state['step'] = 0
                    state['exp_avg'] = torch.zeros_like(p_data_fp32)
                    state['exp_avg_sq'] = torch.zeros_like(p_data_fp32)
                    # look ahead weight storage now in state dict
                    state['slow_buffer'] = torch.empty_like(p.data)
                    state['slow_buffer'].copy_(p.data)
                else:
                    state['exp_avg'] = state['exp_avg'].type_as(p_data_fp32)
                    state['exp_avg_sq'] = state['exp_avg_sq'].type_as(p_data_fp32)

                # begin computations
                exp_avg = state['exp_avg']
                exp_avg_sq = state['exp_avg_sq']
                beta1, beta2 = group['betas']

                # GC operation for conv1d layers and fc layers
                if self.gc_loc:
                    grad = centralized_gradient(grad, use_gc=self.use_gc, gc_conv_only=self.gc_conv_only)
                state['step'] += 1

                # compute variance moving avg
                exp_avg_sq.mul_(beta2).addcmul_(grad, grad, value=(1 - beta2))

                # compute mean moving avg
                exp_avg.mul_(beta1).add_(grad, alpha=(1 - beta1))

                buffered = self.radam_buffer[int(state['step'] % 10)]

                if state['step'] == buffered[0]:
                    n_sma, step_size = buffered[1], buffered[2]
                else:
                    buffered[0] = state['step']
                    beta2_t = beta2 ** state['step']
                    n_sma_max = 2. / (1 - beta2) - 1
                    n_sma = n_sma_max - 2 * state['step'] * beta2_t / (1 - beta2_t)
                    buffered[1] = n_sma
                    if n_sma > self.n_sma_threshold:
                        step_size = math.sqrt(
                            (1 - beta2_t) * (n_sma - 4) / (n_sma_max - 4) * (n_sma - 2) / n_sma * n_sma_max / (n_sma_max - 2)
                        ) / (1 - beta1 ** state['step'])
                    else:
                        step_size = 1. / (1 - beta1 ** state['step'])
                    buffered[2] = step_size

                # if group['weight_decay'] != 0:
                #     p_data_fp32.add_(-group['weight_decay'] * group['lr'], p_data_fp32)

                # apply lr
                if n_sma > self.n_sma_threshold:
                    denom = exp_avg_sq.sqrt().add_(group['eps'])
                    g_grad = exp_avg / denom
                else:
                    g_grad = exp_avg

                if group['weight_decay'] != 0:
                    g_grad.add_(p_data_fp32, alpha=group['weight_decay'])

                # gc operation
                if not self.gc_loc:
                    g_grad = centralized_gradient(g_grad, use_gc=self.use_gc, gc_conv_only=self.gc_conv_only)

                p_data_fp32.add_(g_grad, alpha=-step_size * group['lr'])
                p.data.copy_(p_data_fp32)

                # integrated look ahead
                # we do it at the param level instead of group level
                if state['step'] % group['k'] == 0:
                    # get access to slow param tensor
                    slow_p = state['slow_buffer']
                    # (fast weights - slow weights) * alpha
                    slow_p.add_(p.data - slow_p, alpha=self.alpha)
                    # copy interpolated weights to RAdam param tensor
                    p.data.copy_(slow_p)
        return loss
